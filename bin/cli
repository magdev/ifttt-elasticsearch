#!/usr/bin/env node
/**
 * Copyright (c) 2015, Marco Gr√§tsch <magdev3.0@gmail.com>
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *     * Redistributions of source code must retain the above copyright
 *       notice, this list of conditions and the following disclaimer.
 *     * Redistributions in binary form must reproduce the above copyright
 *       notice, this list of conditions and the following disclaimer in the
 *       documentation and/or other materials provided with the distribution.
 *     * Neither the name of the <organization> nor the
 *       names of its contributors may be used to endorse or promote products
 *       derived from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL <COPYRIGHT HOLDER> BE LIABLE FOR ANY
 * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
 * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

process.env.NODE_CONFIG_DIR = __dirname + '/../config';

var program = require('commander'),
	pckg = require('../package.json'),
	scraper = require('../lib/scraper'),
	strip = require('js-striphtml'),
	config = require('config'),
	elasticsearch = require('elasticsearch'),
	es = new elasticsearch.Client(config.elasticsearch);

function scrape(url) {
	console.log(url);
	es.search({
		index: config.index.name,
        q: url
    }).then(function(body) {
    	body.hits.hits.forEach(function(doc) {
	    	scraper(doc._source.url, function(error, article) {
	    		es.update({
	    			index: config.index.name,
					type: doc._type,
					id: doc._id,
					body: {
						doc: {
							fullpage: strip.stripTags(article.content)
						}
					}
	    		}, function (error, response) {
	    			  console.log(response);
	    		})
	    		doc._source.fullpage = article.content;
	    		console.log(doc);
	    	});
    	});
    }, function (error) {
        console.trace(error.message);
    });
}

program
  .version(pckg.version)
  .option('-i, --index [url]', 'Scrape and index an URL', scrape)
  .parse(process.argv);